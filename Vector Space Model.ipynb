{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Library Import </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load Dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('NewsCategorization/Data_Train.csv',encoding='cp1252')\n",
    "test = pd.read_csv('NewsCategorization/Data_Test.csv',encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Train:  7628\n",
      "Jumlah Test:  2748\n",
      "Total 10376\n"
     ]
    }
   ],
   "source": [
    "print('Jumlah Train: ',np.shape(train)[0])\n",
    "print('Jumlah Test: ',np.shape(test)[0])\n",
    "print('Total',np.shape(train)[0]+np.shape(test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    1246\n",
      "0    1686\n",
      "2    1924\n",
      "1    2772\n",
      "Name: SECTION, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['SECTION'].value_counts().sort_values().plot(kind = 'bar')\n",
    "print(train['SECTION'].value_counts().sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing  Function </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tujuan preprocessing ini adalah untuk \"membersihkan\" data dari huruf/karakter yang tidak diinginkan sehingga output dari data yang baru adalah hanya berupa kalimatnya saja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = re.sub('[~`!@#$%^&*():;\"{}_/?><\\|.,`0-9]', '', text)\n",
    "    text = text.replace('  ',' ')\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    text = text.replace('â€™','')\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Obtaining Vector Function </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi dibawah ini mengambil vektor dari hasil pembangunan vocabulary dari vector space model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=5)) for doc in sents])\n",
    "    return regressors,targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lakukan preprocessing terhadap dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['STORY'] = train['STORY'].apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['STORY'] = test['STORY'].apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> List / Document Assignment </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses dibawah ini adalah melakukan penyimpanan dataset bertipe objek/dokumen dan melakukan tokenisasi kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTrain = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i in range(len(train)):\n",
    "    words = word_tokenize(str(train['STORY'][i]).lower())\n",
    "    tags = [train['SECTION'][i]]\n",
    "    dTrain.append(analyzedDocument(words, tags))\n",
    "dTrain = pd.Series(dTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTest = []\n",
    "analyzedDocument2 = namedtuple('AnalyzedDocument2', 'words tags')\n",
    "for i in range(len(test)):\n",
    "    words = word_tokenize(str(test['STORY'][i]).lower())\n",
    "    tags = [i]\n",
    "    dTest.append(analyzedDocument2(words, tags))\n",
    "dTest = pd.Series(dTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Building Vocabulary of Train Data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses membangun vocabulary dari data train dengan mengeset jumlah epoch (iterasi) sebanyak 20, dimensi vektor sebanyak 5, dan minimum alpha sebanyak 0.0025 dan alpha awal sebesar 0.025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faoezanf/opt/anaconda3/envs/env1/lib/python3.6/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "vec_size = 5\n",
    "alpha = 0.025\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.0025,\n",
    "                min_count=5,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(dTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training Vector Space Model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses training dengan mereduksi alpha sebesar 0.0002 untuk setiap iterasinya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasi : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faoezanf/opt/anaconda3/envs/env1/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasi : 1\n",
      "Iterasi : 2\n",
      "Iterasi : 3\n",
      "Iterasi : 4\n",
      "Iterasi : 5\n",
      "Iterasi : 6\n",
      "Iterasi : 7\n",
      "Iterasi : 8\n",
      "Iterasi : 9\n",
      "Iterasi : 10\n",
      "Iterasi : 11\n",
      "Iterasi : 12\n",
      "Iterasi : 13\n",
      "Iterasi : 14\n",
      "Iterasi : 15\n",
      "Iterasi : 16\n",
      "Iterasi : 17\n",
      "Iterasi : 18\n",
      "Iterasi : 19\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print('Iterasi : {0}'.format(epoch))\n",
    "    model.train(dTrain,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "\n",
    "    model.alpha -= 0.0002\n",
    "\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Separation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses pengambilan vektor dari model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = vec_for_learning(model, dTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pembagian dataset dengan data train sebesar 90%, validasi sebesar 5%, dan test sebesar 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val,X_train_2,y_train_val,y_train_2 = train_test_split(X_train, y_train, test_size=0.1, shuffle=True)\n",
    "X_test,X_valid,y_test,y_valid = train_test_split(X_train_2, y_train_2, test_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contoh isi vektor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24265113, -0.25403947, -1.1557995 ,  0.41428217,  0.30336267],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah separasi\n",
      "===============\n",
      "Data Train :  6865\n",
      "Data Test :  381\n",
      "Data Validasi :  382\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah separasi\")\n",
    "print(\"===============\")\n",
    "print(\"Data Train : \",np.shape(X_train_val)[0])\n",
    "print(\"Data Test : \",np.shape(X_test)[0])\n",
    "print(\"Data Validasi : \",np.shape(X_valid)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Logistic Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah proses training, validating dan testing menggunakan algoritma/classifier Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (Logistic Regression) : 0.96081573197378\n",
      "Validation accuracy (Logistic Regression) : 0.9607329842931938\n",
      "Test accuracy (Logistic Regression) : 0.968503937007874\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train_val, y_train_val)\n",
    "#\n",
    "training_1 = logreg.predict(X_train_val)\n",
    "print('Training accuracy (Logistic Regression) : %s' % accuracy_score(training_1, y_train_val))\n",
    "#\n",
    "y_pred_valid = logreg.predict(X_valid)\n",
    "print('Validation accuracy (Logistic Regression) : %s' % accuracy_score(y_valid, y_pred_valid))\n",
    "#\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "print('Test accuracy (Logistic Regression) : %s' % accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Random Forest </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah proses training, validating dan testing menggunakan algoritma/classifier Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (Random Forest) : 0.9976693372177713\n",
      "Validation accuracy (Random Forest) 0.9476439790575916\n",
      "Testing accuracy (Random Forest) : 0.9553805774278216\n"
     ]
    }
   ],
   "source": [
    "RF_clf = RandomForestClassifier(n_estimators=10, random_state = 42)\n",
    "RF_clf.fit(X_train_val, y_train_val)\n",
    "#\n",
    "training_2 = RF_clf.predict(X_train_val)\n",
    "print('Training accuracy (Random Forest) : %s' % accuracy_score(training_2, y_train_val))\n",
    "#\n",
    "y_pred_valid2 = RF_clf.predict(X_valid)\n",
    "print('Validation accuracy (Random Forest) %s' % accuracy_score(y_valid, y_pred_valid2))\n",
    "#\n",
    "y_pred_test2 = RF_clf.predict(X_test)\n",
    "print('Testing accuracy (Random Forest) : %s' % accuracy_score(y_test, y_pred_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Test Data Prediction </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data,_  = vec_for_learning(model, dTest)\n",
    "prediksi_test1 = logreg.predict(test_data)\n",
    "prediksi_test2 = RF_clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena data test yang asli tidak memiliki label, berikut adalah similarity/kemiripan hasil prediksi yang didapatkan berdasarkan classifier logistic regression dan random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity hasil prediksi berdasarkan 2 classifier : 0.9617903930131004\n"
     ]
    }
   ],
   "source": [
    "print('Similarity hasil prediksi berdasarkan 2 classifier : %s' % accuracy_score(prediksi_test1, prediksi_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
